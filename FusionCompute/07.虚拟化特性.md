## 虚拟化集群特性

虚拟化使用资源的颗粒度更细、更加灵活，将物理资源转变为虚拟资源，虚拟资源能够被被*随意组织、使用*，带来的一些特性，物理资源仅需要提供相应的处理能力

**集群**：将集群下的主机组织起来，向外呈现更大当量的逻辑主机的计算能力。所有放在同一个集群下的主机共享应用到集群上的特性，集群也借助主机成员的能力向外提供服务。集群成员要有一致的共享存储，使用同一个DVS（二层、大二层）通信，且同一个集群内的所有成员主机应尽量承担一样的业务



### 内存复用

1. 内存共享

   物理内存资源不足时，将多个VM中相同的内存区域指向物理内存的某一内存页，hypervisor将此内存页向上提供为共享内存，以此做到**超分配**。共享的内存页必须是**只读**的，如果要写，会在物理内存中新开辟一块空间执行写操作

2. 内存气泡

   内存气泡本质上是提高内存的利用率。hypervisor为每个VM分配的内存空间，在VM中无法得到完全使用，将每个VM中的琐碎内存通过页表指针整合，对外呈现一片未使用的连续的逻辑内存。实际上VM对自身的内存情况毫不知情，所有的调用都是hypervisor调用硬件，如果VM再要使用气泡中的内存时，实际上是VM需要更多的内存了，此时VM会将请求发送给hypervisor，再由hypervisor调用其他的内存供VM使用

3. 内存置换

   内存置换是调用存储扩充内存空间。内存响应速度和存储响应速度是不一样的，服务质量也不一样，为了尽可能提高服务质量，hypervisor将内存中的冷数据放在存储中

```diff
- 实际的内存复用技术指综合运用此3种方案，无论以上哪种方案，VM对自身的内存调用都是无感知的，所有的操作都由hypervisor执行
```



### HA（High Availability）

可用性与可靠性的区别：可用性主要实现**业务连续**，也就是故障发生时能够实现业务不中断或极短时间内恢复业务的特性。可靠性主要站在**预防故障**的视角

*HA*特性针对集群生效，一个集群中的某CNA节点的宕机会被VRM检测到，故障前VRM也检测所有CNA的运行状态，所以VRM能够根据宕机CNA节点上的VM规模在其他CNA节点上重启业务VM，重启后的业务VM仅需重连后端存储即可恢复业务。*HA*能够实现快速的故障重启，业务会中断，用户也有感知

除了针对CNA节点的*HA*，也有针对VM的*HA*，仅针对Windows的蓝屏故障。要实现针对VM的*HA*必须在安装GuestOS后在VM中安装**Tools**工具，此工具由虚拟化软件提供，*Tools*中包括配合hypervisor工作的**监控**软件和**驱动**软件



### DRS（Dynamic Resource Scheduler）

动态资源调度。每个CNA节点都与VRM有管理平面通信，VRM记录各个CNA节点的资源调度情况以便于实现动态资源调度。*DRS*主要指代CNA节点中的**CPU和内存**的负载均衡，**DRS是集群特性，热迁移是DRS的先决条件**

**迁移**

迁移分为*热迁移*和*冷迁移*，*热迁移*下又分为**主机迁移**、**存储迁移**和**两者都迁移**。VM大致可以分为两种资源：**计算资源**、**存储资源**，*计算资源*在CNA节点上，表现为占用CNA节点的vCPU和vMEM。*主机迁移*表示VM所占用的*计算资源*从CNA1节点迁移到CNA2节点上运行。*存储迁移*表示VM存储文件在后端存储节点间迁移，*计算资源*不动

**热迁移（内存分片，迭代迁移）**

VM迁移之前，业务是连续的，执行热迁移时先将VM的内存冻结，保存当前VM内存数据的完整性，内存冻结后向外仅提供只读，但业务在持续进行数据访问，所以必须为此VM新开辟一个内存空间用于临时的业务数据写入，由VRM将首次冻结的内存复制到CNA2节点。内存复制完成后CNA1节点将开辟的临时内存空间做二次冻结并再次另开辟一个临时使用的内存空间，由VRM将二次冻结的内存数据复制到CNA2节点。重复此操作直到某个临界点，将VM在CNA1节点中的内存数据完全复制到CNA2节点上并在CNA2节点上运行VM，这整个过程GuestOS都无感知，由hypervisor执行迁移

```diff
- 使用热迁移必须保证CNA节点处于同一DVS下，且都使用同一共享存储
```

**DRS规则**

基于业务的视角可以对集群中的VM设置DRS规则组，规则组用于实现VM之间的**聚集**策略或**互斥**策略

- 聚集虚拟机：列出的VM必须在同一CNA上运行，一个VM只能加入一条聚集虚拟机规则中

- 互斥虚拟机：列出的VM必须在不同CNA上运行，一个VM只能加入一条聚集虚拟机规则中
- 虚拟机到主机：关联一个VM组和Host组并设置关联规则，指定所选VM组的成员是否能够在特定Host组的成员上运行



### DPM（Dynamic Power Managerment）

DPM基于DRS实现，DRS与DPM都是集群特性，是在CNA之间的热迁移的过程。DRS侧重于根据CNA的CPU、Mem的运行状态实现集群的负载均衡。DPM侧重于集群的**绿色省电**，不同时间段的业务的访问量不同会导致一些CNA节点的轻载，将轻载节点上的业务VM迁移到其他CNA节点使轻载节点变为空载节点，并对其执行下电操作，等到其他CNA节点处于重载运行时再将空载节点上电并执行DRS操作。这就是**轻载合并、空载下电、重载上电**，无论重载或轻载都有一个阈值，VRM通过阈值判断CNA节点处于重载或轻载

```diff
- DRS或DPM的自动迁移仅会在同一集群中迁移，手动迁移可实现跨集群迁移，但仍需要处于同一DVS和共享存储的情况下
- 要实现重载上电必须配置主机BMC参数，通过BMC端口实现CNA节点的上电
```



### NUMA（Non Uniform Memory Access）

**Host NUMA**

一般服务器设备中使用多路多核架构以提高设备的并发处理能力，不同CPU的位置不同与内存条的距离远近也不同。*NUMA*架构将内存条按照CPU的位置进行排列，使每个CPU可以就近的访问离自身最近的内存条，防止不同CPU访问距离较远的内存。每一颗CPU及其周边内存、总线叫做一个NUMA-node，每一个node都是逻辑的计算节点，理论上来说都能承担一个独立的计算作业，有自身独立的CPU和内存和连接其他CPU和内存的总线。硬件资源最终都是由OS调度使用，为了使node内的内存数据也由此node内的CPU处理，必须使OS感知物理架构。NUMA功能需要通过BIOS打开

**GuestOS NUMA**

VM中的应用向GuestOS发起作业，由于GuestOS对硬件架构无感知，GuestOS会将请求直接发送给hypervisor，这就有可能出现CPU处理远距离内存数据。为了避免此情况，需要hypervisor将硬件架构透传给GuestOS，使GuestOS感知NUMA，尽可能实现同一node下的CPU处理内存数据



### Tools

任何hypervisor都会为运行在其上的VM提供相应的*Tools*，*Tools*主要包括**内核态的硬件驱动**和**用户态的vm-agent进程**，*驱动*主要用于VM配合hypervisor完成虚拟化资源调配的操作，*agent*主要用于向VRM报告VM的运行状态



#### 小结

**基本特性**

- 内存复用
- NUMA

**HA**

- CNA主机故障
- 数据存储故障
- Windows蓝屏

**负载均衡**

- 集群资源调度
- 电源管理
- DRS规则

**IMC**

​	不同型号的CPU的指令集会有所不同，两个CNA节点的CPU指令集不同可能导致VM迁移失败，为了避免这种情况，所有CNA节点的CPU都是用低版本的指令集以达到向VM提供同样的指令集，这就是IMC策略，IMC策略使集群特性，且开启IMC特性时集群中不能存在VM

```diff
+ VNC登录：管理员通过浏览器耦合在http上的VNC协议访问到CNA主机，再通过hypervisor提供的API访问到VM，将VM的控制台复制到浏览器
```

