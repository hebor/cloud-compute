# 虚拟化集群特性

虚拟化带来的价值即使用资源的颗粒度更细、更灵活，虚拟化通过软件技术提取硬件基础设施的物理资源、功能，并切片整合为抽象的虚拟资源池，当有新增的虚拟机业务产生时，从资源池中抽取虚拟化资源分配给虚拟机，没有虚拟机业务产生时，虚拟化资源仍存放在资源池中等待调用，资源池的量越大，能支撑的虚拟机业务就越多。将物理资源转变为虚拟资源，虚拟资源具备物理资源所不具备的灵活性，虚拟资源能够被被随意组织、使用它带来的一些特性，物理资源仅需要提供相应的处理能力即可

然而，当应用程序对性能有高要求时，仍建议直接使用物理硬件资源。虚拟机对虚拟硬件资源的调用，实际上还是需要经过Hypervisor调度分配到物理硬件资源，看起来是虚拟机调用虚拟硬件资源，实际上最终还是需要调用物理硬件的能力，这个过程中还需要Hypervisor进行调用，在灵活性和特性上更加丰富，但性能上却不如直接使用物理硬件资源

## 集群特性

在华为FusionCompute虚拟化环境的组织结构中，缺省情况下资源池的顶层结构是`Site`，在`Site`下面可以创建集群文件夹、创建集群结构、直接添加CNA节点，多个集群结构可以存放在同一个集群文件夹中，集群文件夹作为集群结构的组织单元、集群结构本身也只是作为多个CNA节点的组织单元，因此无论是集群文件夹或集群结构都不是必须要创建的，VRM所管控的仍是CNA节点，因此`Site`下面可以直接接入CNA节点。然而，**华为针对虚拟化环境中启用虚拟化特性时，大部分时候是以集群为单位启用虚拟化特性的**，在同一个集群下的所有CNA节点能够享受到应用在集群上的虚拟化特性

一个集群能够将其下面的多个CNA节点组织起来，向外呈现更大当量的逻辑主机的计算能力。所有放在同一个集群下的主机共享应用到集群上的特性，集群也借助主机成员的能力向外提供服务。在组建一个集群时，建议集群下的所有成员要使用相同的资源、相同的配置、相同的架构和规格，例如使用相同的共享存储、使用同一个DVS（二层、大二层）通信，且同一个集群内的所有成员主机应尽量承担一样的业务，这样集群内的虚拟机在迁移或某些虚拟化特性失败的情况

与集群不一样的是，集群文件夹本身不具备任何特性，似乎集群文件夹的存在只是为了让管理集群结构变得更优雅

### 内存复用

业界的内存虚拟化技术的实现都离不开以下三种技术，可能还有更多的技术用于实现内存虚拟化，但无论是基于哪种技术实现的内存虚拟化，最终的目的都是为了实现让有限的物理内存，通过内存虚拟化特性得到更多的可用内存

![虚拟化集群特性-内存复用](./image/HCIA/虚拟化集群特性-内存复用.png)

如图，通过内存复用技术可以实现，在物理内存只有4G空间的情况下，实现3台虚拟机的内存总和为6G。内存复用技术实际上就是一种变相的超分配，使表面上看起来可用的内存超过实际的内存容量

1. 内存共享

   三个虚拟机都希望自己有独享的2G内存可以用于读写操作，由Hypervisor向每个虚拟机提供配备给虚拟机的2G内存，但物理内存只有4G，因此需要通过Hypervisor来提高物理内存的利用率，使Hypervisor表面上看起来能够提供6G内存

   当物理内存资源不足时，Hypervisor会将多个VM中相同的内存区域指向物理内存的某一内存页，Hypervisor将此内存页向上提供为共享内存，以此做到**超分配**。共享的内存页必须是**只读**的，如果要写，会在物理内存中新开辟一块空间执行写操作

   华为的内存共享技术所提供的共享内存，都是所有虚拟机中未使用到的内存空间。例如，假设三个VM上各自实际在运行的内存空间都只占用了1G内存，那么三个虚拟机另一半未占用的1G空间会被Hypervisor映射到物理内存的某一内存页

2. 内存气泡（别名内存气球）

   无论是内存共享或内存气泡，实际上都是超分配，内存气泡本质上是提高内存的利用率。Hypervisor为每个VM分配的内存空间，一般在VM中都无法得到完全使用，例如分配给VM的2G内存空间实际仅使用了70%，Hypervisor会将每个VM中的剩余的30%的琐碎内存通过页表指针整合，对外呈现一片未使用的连续的逻辑内存，然后由Hypervisor将这部分逻辑内存再分配给新的虚拟机，这就是内存气泡机制

   实际上VM对自身的内存情况毫不知情，所有的调用都是Hypervisor调用硬件，如果VM再要使用气泡中的内存时，实际上是VM需要更多的内存了，此时VM会将请求发送给hypervisor，再由hypervisor调用其他的内存供VM使用

3. 内存置换

   内存置换是调用存储空间用于扩充内存空间。内存响应速度和存储响应速度是不一样的，其提供的服务质量也不一样，为了尽可能提高服务质量，Hypervisor会将内存中的冷数据放在存储空间中

实际的内存复用技术指综合运用此3种方案，可能某一时刻使用内存复用，或某一时刻同时使用内存共享、内存气泡，但无论以上哪种方案，VM对自身的内存调用都是无感知的，所有的操作都由Hypervisor执行，管理员也无法选择具体使用其中某一种方案，管理员所做的也只是勾选了内存复用技术选项

针对集群开启的内存复用特性，能够享受到内存复用特性的虚拟机必须是同一台CNA节点上的虚拟机，不可跨CNA节点享受到内存复用特性。例如，CNA_01节点上通过内存气泡技术提供了更高的虚拟机密度，但CNA_01节点上的VM无法获取到CNA_02节点上通过内存气泡技术提供的内存空间

### HA（High Availability）

HA直译为高可用，在如今的业务软件上能够经常看到不同厂家强调自家产品的可用性和可靠性，这两种特性从两个不同的维度描述了厂家的设计方案，其中高可用就是研究产品的可用性

#### 可用性与可靠性

可用性主要关注两个方面：业务的连续性和业务的中断时间。它最关心的是当业务发生故障/灾难时，如何能够尽可能确保业务不中断，因此如果存在一套冗余的业务网络，在当前业务网络故障时，冗余的业务网络能够在有限的时间内快速支撑业务的正常运转，那么这套系统就是具备足够的可用性的。从冗余的网络支撑业务的正常运转所耗费的时间越短，甚至于业务不产生中断，那么这套系统的可用性就越高，但同样成本也就越高

而可靠性会更关注细节的实现，可靠性主要站在预防故障的视角，通过冗余的设计，例如通过堆叠、集群等技术来提高业务系统的可靠性。可用性侧重于整体业务的宏观视角，可靠性更关注网络结构中的某个微观的组件部分，例如某条链路、某个设备等

#### HA特性

HA特性针对集群生效，VRM通过心跳监控着所有CNA节点的状态，它也知道每一台CNA节点上存在多少台、何种规模的VM，因此当一个集群中的某CNA节点因未知原因宕机时，首先故障的CNA节点会被VRM检测到，其次VRM能够根据宕机的CNA节点上的VM规模在集群内的其他多个CNA节点上均衡的分布、重启业务VM，重启后的业务VM仅需重连后端存储即可恢复业务。HA能够实现快速的故障重启，但出于成本的考虑，大多数情况下业务会中断，用户也有感知，HA的价值在于它能够比管理员更快的发现、处理业务VM的故障，在集群内发生的故障，集群自身具备一定的“自愈”能力，免除了管理员的手动干预

针对集群的HA特性实际上也就是实现了该集群下的CNA节点的HA，除了CNA节点的HA，也有VM的HA，针对VM的HA只考虑Windows的蓝屏故障这一种场景，其它VM场景不支持。要实现针对VM的HA必须在GuestOS上安装**Tools**工具，此工具由虚拟化软件提供，Tools中包括配合Hypervisor工作的**监控**软件和**驱动**软件。一般情况下，Hypervisor只能监控计算节点的状态，而无法监控到计算节点上的VM状态是否健康，以VRM与CNA为例，正常情况下，当某个CNA节点上的VM出现异常时，是VM上的GuestOS出现异常，不会影响VM所在的CNA节点本身的状态，VRM缺省情况下仅管理CNA节点，CNA节点状态正常，则VRM不会对该CNA节点、以及该CNA节点之上的VM做任何操作

为了实现Hypervisor能够细化到可以监控VM的状态、以及直接对VM执行某些操作，需要尽早的在GuestOS中安装好Hypervisor提供的Tools。无论虚拟化环境中是使用的哪个厂商的Hypervisor，Hypervisor上都一定提供了对应的Tools，该Tools用于安装在VM中，Hypervisor通过Tools在GuestOS中植入了配合虚拟化特性和功能的一些插件，便于Hypervisor对VM的控制力度、控制深度能够深入到GuestOS上。以VM的重启操作为例，重启指令本身是GuestOS的指令，因此重启指令必须是进入GuestOS之后发起，在GuestOS上安装Tools后，等同于是在GuestOS上安装了一个VRM的Agent，通过VRM向VM发起重启操作时，VRM的指令会先发送到CNA的Hypervisor上，再通过Agent向GuestOS发起重启指令。如果直接在Hypervisor上对没有安装Tools的VM执行重启操作，会提示报错

当Windows蓝屏时，VRM可以通过Agent获取到VM的启动状态，此时就可以通过VRM来决定是否对VM开启HA特性，VRM检测到VM蓝屏时可以选择三种处理方式：重启、关机、在其他CNA节点上重启启动一台相同规模的VM

#### HA配置

1. [Site] -> [创建集群] -> [基本信息]；创建一个集群结构。在创建集群时，集群下一台CNA节点都没有，而在创建集群的过程中就需要确认是否启用虚拟化特性，所以虚拟化特性实际上是针对集群生效的

   ![创建集群-1](./image/HCIA/创建集群-1.png)

2. [基本配置]；配置集群策略

   ![创建集群-2](./image/HCIA/创建集群-2.png)

3. [HA配置] -> [故障和响应策略]；

   ![创建集群-3](./image/HCIA/创建集群-3.png)

   故障和响应策略上包含三大策略配置：主机故障处理策略、主机数据存储故障处理策略、虚拟机故障处理策略，每个策略下又包含多个选项，不同选项实现不同的集群特性功能

   - 主机HA故障处理策略；即CNA节点故障后的策略动作
     - 原主机恢复虚拟机：等待故障CNA节点恢复，故障CNA节点恢复之前，该CNA节点上的VM也不会在集群内的其他CNA节点上启动
     - HA虚拟机：将故障CNA节点上的VM负载均衡到集群内的其他CNA节点上进行启动
   - 主机数据存储故障策略；主机数据存储故障指的是该CNA节点无法正常连接到SAN存储，即该CNA的数据存储故障，不是指SAN存储设备故障
     - 不处理：CNA节点无法正常连接到SAN存储时，不对CNA节点做任何动作，等待CNA节点恢复连接状态
     - HA虚拟机：CNA节点无法正常连接到SAN存储时，将该CNA节点上的VM，在集群内其他能够正常连接到SAN网络的CNA节点上重新启动
   - 虚拟机HA故障处理策略；虚拟机的GuestOS上必须要先安装有Tools之后，才能实现HA故障处理策略
     - 不处理：不对VM做任何动作
     - 重启虚拟机：在原CNA节点上重启VM
     - HA虚拟机：在集群内的其他CNA节点上重新启动VM
     - 关闭虚拟机：关闭VM

4. [HA配置] -> [接入控制] -> [HA资源预留]；缺省不启用接入控制特性，HA资源预留特性决定是否在CNA节点上预留计算资源，用于HA恢复故障虚拟机

   ![创建集群-4](./image/HCIA/创建集群-4.png)

5. [HA配置] -> [接入控制] -> [集群允许主机故障设置]；集群内指定数量的CNA节点发生故障时，允许集群通过保留的资源量对故障CNA节点上的VM做故障恢复动作

   ![创建集群-5](./image/HCIA/创建集群-5.png)

6. [HA配置] -> [群体性故障控制]；缺省不启用群体性故障控制

   群体性故障控制需要在“主机相关故障和响应策略”开启时生效，通过配置“故障控制时间”内“允许处理虚拟机HA的主机数”，实现控制群体性故障时出现大规模的虚拟机HA。被群体性策略控制的主机，其上的虚拟机只能等待主机恢复后重启虚拟机。例如，2小时内连续故障了5个主机，缺省情况下的系统配置，只会对前2个主机上的虚拟机按“主机故障处理策略”和“虚拟机代替项“的配置进行HA，其他3个主机上的虚拟机就只能等待主机恢复后重启虚拟机

   ![创建集群-6](./image/HCIA/创建集群-6.png)

7. [计算资源调度配置]；使用缺省参数，暂时不做调整

   ![创建集群-7](./image/HCIA/创建集群-7.png)

8. [IMC配置]；使用缺省参数，暂时不做修改

   ![创建集群-8](./image/HCIA/创建集群-8.png)

9. [虚拟机启动项]；使用缺省参数，暂时不做修改

   ![创建集群-9](./image/HCIA/创建集群-9.png)

#### VM的HA配置

在创建集群的过程中，在HA的故障和响应策略下，虚拟机的故障和响应策略缺省情况下的策略动作是不处理，因此，除非是整个CNA节点故障，否则CNA节点上的虚拟机即便产生了故障，触发的策略动作也是不处理。实现虚拟机的HA有两种方式，一种是在集群的HA特性下，修改虚拟机的故障和相应策略，这种方式是针对集群下的所有虚拟机生效；另一种就是当集群开启HA配置或者计算资源调度时，通过设置虚拟机替代项，对单个或批量虚拟机设置异于集群的HA策略或计算资源调度策略

![VM的HA-1](./image/HCIA/VM的HA-1.png)

1. [虚拟机] -> [操作] -> [Tools] -> [挂载Tools]；为虚拟机挂载Tools软件

   ![安装Tools-1](./image/HCIA/安装Tools-1.png)

2. [集群] -> [配置] -> [虚拟机代替项] -> [添加] -> [勾选指定虚拟机] -> [修改虚拟机故障处理策略]；对单个或批量虚拟机设置异于集群的HA策略或计算资源调度策略

   ![VM的HA-2](./image/HCIA/VM的HA-2.png)

### DRS（Dynamic Resource Scheduler）

集群中的每个CNA上都有虚拟机，每个CNA节点上的虚拟机的密度、每个虚拟机的业务量都是不同的，这势必会导致一个集群下的不同CNA节点之间的负载各有高低，DRS动态资源调度特性就是为了防止产生这种情况，DRS会周期性检查集群内主机的负载情况，根据集群中每个CNA节点的资源消耗量，动态的在集群主机中迁移虚拟机，来达到集群中每个CNA节点的负载接近一致，即负载均衡。集群的自动化级别有两种：自动和手动，在自动调度模式下，虚拟机迁移的整个过程无需管理员手动干预，启用DRS动态资源调度即可自动实现集群内的负载均衡。

所有CNA节点与VRM之间都存在一个管理平面网络，VRM通过管理平面网络记录有所有CNA节点的资源用量，以便于实现动态资源调度特性。DRS所实现的负载均衡，主要是指各个CNA节点的**CPU和内存**资源的负载均衡。当集群中某一个CNA节点的负载过大时，该CNA节点上的部分虚拟机会自动的迁移到集群中的其他CNA节点上，这个过程中势必会使用到迁移技术，同时**热迁移是DRS的先决条件**

整个动态迁移的过程都是**由VRM触发**，可以选择用于触发动态迁移的条件有3种

1. 根据各CNA节点的CPU和内存的使用率作为判断条件
2. 仅根据各CNA节点的CPU使用率作为判断条件
3. 仅根据各CNA节点的内存使用率作为判断条件

#### 迁移技术

迁移分为*热迁移*和*冷迁移*，热迁移是虚拟机在正常运行的状态下实现迁移，迁移过程中保持业务不中断、用户无感知，反之，冷迁移则需要先将虚拟机关机，然后执行虚拟机文件的迁移。实现热迁移需要满足两个先决条件：**集群CNA节点处于同一DVS下、集群CNA节点使用同一个共享存储**，基于这两个先决条件的前提下，热迁移又分为三种模式：**主机迁移**、**存储迁移**、**主机和存储都迁移**

一个VM大致可以看作是两种资源的组合：**计算资源+存储资源**，计算资源落在CNA节点上，表现为占用CNA节点的vCPU、vMEM；存储资源落在后端存储设备上，表现为LUN与数据存储的关联关系

- 主机迁移：表现为VM的计算资源在不同的CNA节点之间迁移，即vCPU、vMEM的迁移，例如VM所需要的计算资源从CNA1节点迁移到CNA2节点上运行，存储资源保持在后端的某个LUN上不变
- 存储迁移：表现为VM的存储资源在不同的LUN之间迁移，即磁盘文件的迁移，例如VM的磁盘文件从后端存储节点的LUN1迁移到LUN2上，计算资源保持在原CNA上不变
- 主机和存储迁移：两者都进行迁移

**主机热迁移（内存分片，迭代迁移）**

整个热迁移的过程中VM都需要正常对外提供业务服务， 在存储资源不变的情况下，计算资源的迁移难点在于内存数据的迁移。VM迁移前的源CNA节点与迁移的目标CNA节点，通过共享存储可以实现两者都能够实时访问到VM的硬盘文件，但VM在不间断对外提供业务服务的过程中，势必会不断产生新的数据操作，热迁移的难点就在于如何通过这些新的业务数据

VM在迁移之前，源CNA节点上VM的内存数据会被冻结，保障当前VM内存数据的完整性，内存冻结后仅向外提供只读，但业务在持续进行数据访问，所以必须为此VM新开辟一个内存空间用于临时的业务数据写入，由VRM将首次冻结的内存复制到目标CNA节点。首次内存复制完成后，源CNA节点将开辟的临时内存空间做二次冻结并再次另开辟一个临时使用的内存空间，由VRM将二次冻结的内存数据复制到CNA2节点。

重复上述操作直到某个临界点，业务一定会执行一次暂停服务，只不过这个服务暂停的时间会非常短暂，甚至于使用户感知不到服务发生过暂停，业务服务暂停后内存数据不会再产生变化，此时VRM再将VM在源CNA节点中的内存数据完全复制到目标CNA节点上并在目标CNA节点上运行VM，这整个过程GuestOS都无感知，由hypervisor执行迁移

为了实现热迁移之后的VM的IP、MAC地址都保持完全不变，要求源CNA节点与目标CNA节点处于同一DVS下，如此才能保障迁移前后的VM的网络配置完全一致。此前提及过，集群的自动化级别分为自动和手动，DRS动态资源调度与手动调度的共同点，是都需要源CNA节点与目标CNA节点处于同一DVS下、使用同一个共享存储，两者的区别在于，DRS动态资源调度迁移虚拟机时，只会在集群内的CNA节点之间迁移虚拟机，而手动迁移则可以在不同集群之间迁移虚拟机

#### DRS规则组

在某些场景下需要对虚拟机的迁移做出一些限制，例如限制某两台主备模式的业务虚拟机不能同时迁移在同一CNA节点上，此时就可以通过DRS规则组实现。基于业务的视角可以对集群中的VM设置DRS规则组，规则组用于实现VM之间的**聚集**策略或**互斥**策略

- 聚集虚拟机：列出的VM必须在同一CNA上运行，一个VM只能加入一条聚集虚拟机规则中

- 互斥虚拟机：列出的VM必须在不同CNA上运行，一个VM只能加入一条聚集虚拟机规则中
- 虚拟机到主机：关联一个VM组和Host组并设置关联规则，指定所选VM组的成员是否能够在特定Host组的成员上运行

#### DPM（Dynamic Power Managerment）

DPM动态电源管理基于DRS实现，因此启用DPM特性的前提是DRS特性必须是启用状态，DRS与DPM实际上依靠热迁移实现的集群特性，是在CNA之间的热迁移的过程。DRS侧重于根据CNA的CPU、Mem的运行状态实现集群的负载均衡。DPM侧重于集群的**绿色省电**，不同时间段的业务的访问量不同会导致一些CNA节点的轻载，将轻载节点上的业务VM迁移到其他CNA节点使轻载节点变为空载节点，并对其执行下电操作，等到其他CNA节点处于重载运行时再将空载节点上电并执行DRS操作。这就是**轻载合并、空载下电、重载上电**，无论重载或轻载都有一个阈值，VRM通过阈值判断CNA节点处于重载或轻载

DRS或DPM的自动迁移仅会在同一集群中迁移，手动迁移可实现跨集群迁移，但仍需要处于同一DVS和共享存储的前提下。要实现重载上电必须配置主机BMC参数，通过BMC端口实现CNA节点的上电

#### DRS配置

此示例配置中未启用DPM特性，DPM特性的配置与DRS的配置相似，选择自动化级别、设置电源管理阈值、设置阈值周期

1. [集群] -> [配置] -> [编辑] -> [计算资源调度配置]；基于已创建的集群重新修改DRS策略配置

   ![DRS-1](./image/HCIA/DRS-1.png)

2. [集群] -> [配置] -> [计算资源调度配置]

   ![DRS-2](./image/HCIA/DRS-2.png)

3. [集群] -> [配置] -> [DRS规则] -> [主机组] -> [添加]；添加主机组

   ![DRS-3](./image/HCIA/DRS-3.png)

4. [集群] -> [配置] -> [DRS规则] -> [虚拟机组] -> [添加]；添加虚拟机组

   ![DRS-4](./image/HCIA/DRS-4.png)

5. [集群] -> [配置] -> [DRS规则] -> [规则组] -> [添加]；添加规则组

   ![DRS-5](./image/HCIA/DRS-5.png)

   每个集群中最多添加20个规则组，规则组有4种类型：

   - 聚集虚拟机：列出的虚拟机必须在同一主机上运行，一个虚拟机只能被加入一条聚集虚拟机规则中。
   - 互斥虚拟机：列出的虚拟机必须在不同主机上运行，一个虚拟机只能被加入一条互斥虚拟机规则中。
   - 虚拟机到主机：关联一个虚拟机组和主机组并设置关联规则，指定所选的虚拟机组的成员是否能够在特定主机组的成员上运行。
   - 虚拟机组到虚拟机组：设置两个虚拟机组之间的依赖关系，指定低优先级虚拟机组依赖于高优先级虚拟机组，虚拟机批量启动时优先启动高优先级虚拟机组内的虚拟机。

6. [集群] -> [配置] -> [DRS规则] -> [均衡组] -> [添加]；添加均衡组

   ![DRS-6](./image/HCIA/DRS-6.png)

   每个集群最多只能添加1个均衡组。用户可以在均衡组中添加若干当前集群下的虚拟机。经过资源调度，在集群内的整体负载均衡的基础上，均衡组内的虚拟机默认以负载均衡的方式在当前集群内也达到均衡分布。例如，同一个集群中部署了两种不同的业务，为了让两种业务均稳定运行，可将两种业务的虚拟机加入均衡组中，这样便可基于业务实现负载均衡。

7. [集群] -> [计算资源调度] -> [立即调度]；手动调度负载均衡

   ![DRS-7](./image/HCIA/DRS-7.png)

### NUMA（Non Uniform Memory Access）

NUMA属于集群特性，但不属于虚拟化特性

**Host NUMA**

一般物理服务器设备中使用多路多核架构，多路即多颗CPU、多核即一颗CPU的多个核心，服务器的多个CPU并发处理作业，以提高设备的并发处理能力，CPU数量越多则并发处理能力越强，然而CPU主要负责计算机系统的计算、控制等功能，它需要从内存中访问程序与数据，不同CPU的位置不同与内存条的距离远近也不同。

NUMA架构将内存条按照CPU的位置进行排列，使每个CPU可以就近的访问离自身最近的内存条，防止不同CPU访问距离较远的内存。每一颗CPU及其周边内存、总线叫做一个NUMA Node，每一个Node都是逻辑的计算节点，服务器内部以NUMA Node为单元进行分割，理论上来说每个Node都能承担一个独立的计算作业，有自身独立的CPU和内存和连接其他CPU和内存的总线。

实际上每个CPU都应该能访问到所有的内存条，只不过从性能上来说，CPU会优选自身所处的Node内的内存，次选使用Node间的内存，即远端内存。然而硬件资源最终都是由OS调度使用，因此为了实现Node内的内存数据也由此Node内的CPU处理，必须先使OS感知物理架构，即在BIOS设置中打开NUMA功能

**GuestOS NUMA**

在虚拟化场景中，GuestOS与Hypervisor不一样，通过BIOS设置能够使Hypervisor感知到底层NUMA架构，但GuestOS无法感知到NUMA Node，所以当VM中的应用向GuestOS发起作业时，GuestOS会将作业推送给Hypervisor，如果此时Hypervisor支持NUMA架构，那么由Hypervisor给不同的Node分配作业，但这仍可能出现CPU处理远距离内存数据。现在我们希望Hypervisor能够把底层的NUMA Node信息透传给GuestOS，使GuestOS能够感知到Hypervisor提供的底层的虚拟硬件资源是NUMA架构的，这就是GuestOS NUMA

创建虚拟机时，缺省情况下VM能够感知到硬件上有多少个CPU，但是VM感知不到NUMA架构，所以GuestOS在分配作业时，它在分配内存和CPU的指令和数据时，不会按照Node的位置关系来分配，所以GuestOS在分配作业时可能会将同一个作业的不同部分分配到两个不同的Node上。GuestOS NUMA特性，就是Hypervisor将底层的NUMA结构透传给GuestOS，GuestOS在分配作业时就能够清楚的知道作业该分配给哪个Node，再将虚拟机所认知的虚拟Node映射到物理Node，避免GuestOS的作业分配到不同Node之间

GuestOS NUMA特性是为了让GuestOS在分配作业时，能够尽可能的享受到NUMA架构的优点，在源端分配作业时就能够考虑到Node架构对性能的影响

#### NUMA配置

[集群] -> [配置] -> [编辑] -> [基本配置]；启用集群GuestOS NUMA特性

![GuestOS NUMA](./image/HCIA/GuestOS NUMA.png)

### IMC

一般情况下在规划设计时项目中会使用相同规模、相同架构的服务器，但实际上可能会由于一些历史原因或利旧服务器，导致出现不同规模的服务器。不同型号的CPU的指令集会有所不同，然而这并不会影响到FC虚拟化平台上创建的虚拟机，但问题在于CNA节点故障或VM故障时，虚拟机的迁移可能会产生问题，两个CNA节点的CPU指令集不同可能导致VM迁移失败。

为了避免这种情况，应该设置整个集群内所有异构的、不同型号的CPU都需要使用同样的、兼容的指令集，无论CNA节点的CPU是否为最新产品，集群内所有的CPU使用低版本的指令集以达到向VM提供同样的指令集，这就是IMC策略。

IMC可以确保集群内的主机向虚拟机提供相同的CPU功能集，即使这些主机的实际CPU不同，也不会因CPU不兼容而导致迁移虚拟机失败。目前IMC策略仅支持Intel不同型号CPU的热迁移，其他厂商的CPU不能配置该功能

设置集群IMC策略时，如果集群中有主机或虚拟机，则必须满足下面的条件：

- 集群下主机的CPU功能集必须等于或高于设置的目标基准功能集。
- 集群下运行或休眠状态的虚拟机CPU功能集必须等于或低于目标基准功能集。如果存在不满足条件的虚拟机，需要将该虚拟机关机或迁移出该集群后设置。

如果VM不能匹配当前集群主机的指令集，VM要么无法工作、要么迁移无法完成，此时只能删除原VM并重新创建VM，或将VM迁移到其他集群中，因此建议在创建集群时先设置好集群特性，然后再创建VM

## Tools

任何的虚拟化软件Hypervisor都会为运行在其上的VM提供相应的Tools，Tools主要包含**内核态的硬件驱动**和**用户态的vm-agent进程**，驱动程序主要用于VM配合Hypervisor完成更好的I/O吞吐、快照、热迁移等跟虚拟化资源相关的内核态的调配操作，它可以用于获取虚拟机的硬件信息；在虚拟化场景下，整个VM都处于用户态，因此处于GuestOS上的vm-agent，即用户态的vm-agent进程，vm-agent主要用于向VRM报告VM的运行状态信息，例如获取虚拟机的IP、获取虚拟机的状态、虚拟机软关机等操作

虽然VM运行在FusionCompute虚拟化平台上，但实际上FC平台的一些特性与VM并没有直接的关系，例如迁移、快照、在线调整虚拟机的规格等特性都是FC平台的特性，如果管理员想对VM实现在线调整，就必须在VM中安装Tools工具，Tools工具中包含的内核态的硬件驱动就是配合Hypervisor来提供虚拟化特性的

> **补充：VNC登录**
>
> 管理员通过浏览器耦合在http上的VNC协议访问到CNA主机，再通过hypervisor提供的API访问到VM，将VM的控制台复制到浏览器
