# 存储基础

## 传统存储方式

传统存储指的是非虚拟化存储。一般情况下能够提供存储能力的设备，比较常见的有两种：阵列（又称为Array）、存储服务器，这两种设备在数据中心和云计算场景下使用的比较多，其中，阵列是**集中式存储**中的典型设备、存储服务器是**分布式存储**中的典型设备

阵列结构分为**控制框**和**硬盘框**，虽然两者都被称为”框“，但实际上两者都是独立的硬件设备，控制框包括**控制模块**和**硬盘**，在条件允许的情况下，仅通过控制框即可实现业务数据的存储，当控制框设备的磁盘容量不够时，可以通过新增硬盘框设备进行磁盘容量扩容。硬盘框包括**级联模块**和**硬盘**，控制框设备通过总线（Mini_SAS）与硬盘框设备的级联模块连接，实现扩展存储总容量

控制框上的控制模块对外提供端口，通过控制模块的端口接入管理网络和业务网络，控制模块所接入的业务网络就是SAN（Storage Area Network，存储区域网络），SAN用于对物理主机、服务器等设备提供存储资源，控制模块提供的用于与服务器的存储接口进行I/O收发的，都是高速端口，例如10G以太网口、FC（Fiber Channel，光纤通道）端口等，每个控制框上的控制模块都是双活双控的结构，又称为A、B控，双活双控的结构是为了保障阵列最基本的高可用性，也有四控、八控的设备

相比较阵列，存储服务器就可以说是”平平无奇“，与普通的服务器相比，存储服务器似乎也只是硬盘更多一些。将多台存储服务器聚在一起形成一个存储池，存储服务器的数量越多，存储池的容量越大，从存储容量上来看，存储池与阵列都能够提供大容量的存储服务，然而，业务服务器的数据最终会落在存储池中的某个存储服务器上的某块磁盘上，因此存储服务器所组成的存储池的方式，也被称为分布式存储

无论是什么形态的存储设备，数据最终都会落到磁盘上，这也是阵列和存储服务器相同的特征。阵列在可靠性上比存储服务器要好，但阵列的控制框的控制模块（又称为机头）可能会成为瓶颈，前端的计算节点或后端的硬盘数量都可以持续扩大，但机头的硬件I/O速率无法扩展。存储服务器作为分布式存储提供了高并发的特性，不会有I/O瓶颈的问题，但存储服务器的多副本机制生成的冗余数据量远超阵列

副本：根据副本数量的设置将同一份数据复制并存储到不同的存储节点。比如3副本，一个数据到达存储，这个数据要复制为3个副本分别存放到不同的3台存储服务器上

### 物理磁盘类型

1. SATA（Serial Advanced Technology Attachment）：Serial ATA口的硬盘又称为串口硬盘，SATA采用串行连接方式，具备更强大的纠错功能，串行接口还具备结构简单、支持热插拔的优点。SATA硬盘本身并不是高速硬盘，因此一般被用于个人PC上，但由于其支持热插拔等功能，SATA硬盘也可以被于阵列上，使用SATA硬盘的场景一般是用于对I/O速率要求不高的归档场景下，使用SATA硬盘存储长期存放的数据。ATA口硬盘是早期IDE接口的硬盘，IDE硬盘采用并行连接方式，无法用做高速传输，总线之间需要对齐时钟，因此还需要使用特殊编码，因此现在都是用Serial ATA硬盘

2. SAS（Serial Attached SCSI）：即串行连接SCSI，是新一代的SCSI技术，采用串行连接方式获得更高的传输速度，并通过缩短连接线改善内部空间。SCSI口采用并行连接方式，SAS是并行SCSI接口之后开发出的全新接口，此接口的设计是为了改善存储系统的效能、可用性和扩充性，并提供与SATA接口硬盘的兼容性

3. NL-SAS（Near Line SAS）：采用了SAS的磁盘接口和SATA的盘体，作为SAS和SATA的综合体，NL-SAS硬盘的转速只有7200转，因此性能比SAS硬盘差，但由于使用了SAS接口，因此在寻址和速度上有了提升

4. SSD（Solid State Disk）：固态硬盘，使用固态电子存储芯片阵列制作而成的硬盘，由控制单元和存储单元（FLASH芯片、DRAM芯片）组成。SSD在接口的规范和定义、功能及使用方法上与普通硬盘的完全相同，在产品外形和尺寸上也完全与普通硬盘一致。SSD虽然具有传统机械硬盘所不具备的快速读写、质量轻、能耗低以及体积小等特点，但其使用寿命有限且价格较高。

SATA、SAS、NL-SAS三种硬盘都是机械硬盘，其本质上都是“磁盘”，其核心存储介质是覆盖磁性材料的盘片，机械硬盘内部通过机械臂摆动磁头，通过磁头改变盘片上磁性材料的极性方向（例如正负磁化）实现写入和读取，这一过程完全基于磁效应，因此机械硬盘在电磁干扰的环境中可能会出现问题。相比较之下，固态盘脱离了磁盘的性质，不受电磁干扰的影响

机械硬盘面对固态硬盘时仍具备其优势，当硬盘出现损坏时，机械硬盘的盘片由于通过磁性材料实现读写，那么只要机械硬盘的盘片上的扇区的磁性材料仍然完整，那么数据就有被修复的可能，而固态硬盘由于使用电路结构存储数据，一旦固态硬盘损坏，就意味着数据无法挽救

### RAID

针对于“阵列”和“存储服务器”，两者使用的RAID技术有所不同，阵列基于**块级虚拟化技术**实现RAID，也称为`RAID 2.0+`，存储服务器基于整个硬盘实现RAID，将存储服务器上的多个硬盘组合为RAID。在计算虚拟化章节中曾提及，虚拟化的形态分两种：多虚一、一虚多，无论是哪一种RAID，其所实现的效果简单一点说，就是将多个物理硬盘组成一个集合，形成一块逻辑硬盘的过程，因此RAID技术本身也是虚拟化技术的一种，只不过一般基于硬盘的RAID不会刻意提及虚拟化的概念

- 存储服务器的RAID

    Disk RAID，基于磁盘的RAID，将多个物理磁盘组合为一个逻辑硬盘

- 阵列的RAID

    块RAID，基于块级虚拟化的RAID，也称为`RAID 2.0+`，`RAID 2.0+`首先会将阵列中的每个硬盘的存储空间切块（称为chunk），然后在不同硬盘的中各划分出一个块空间组合为一个RAID（称为chunk group）。块级RAID中，一个硬盘上被划分为N个块存储空间，每个块又可能属于不同的chunk group，当该硬盘损坏时，会牵扯到所有使用了此坏盘块的chunk group，所有被牵扯到的chunk group又会通过其他在group中的块所在的磁盘，帮助坏盘修复数据。宏观上看就是大部分磁盘都帮助坏盘修复数据，因此`RAID 2.0+`无论是在读写或数据重构上，性能都上远优于Disk RAID

    由于阵列的特性，硬盘容量不足时通过级联模块新增连接硬盘框即可实现硬盘容量扩容，因此阵列中不需要考虑磁盘不足的情况

基于存储服务器和阵列各自的RAID特性，Disk RAID基于硬盘配置RAID组时，前提条件是一个RAID组下的物理硬盘必须是相同的硬盘类型，例如一个Disk RAID组下的所有物理硬盘都是SATA盘，另一个Disk RAID组下的所有物理硬盘都是SAS盘；阵列的RAID组则不计较底层硬盘类型，不同的硬盘类型可以组在同一个RAID组中

#### RAID分类

RAID（Redundant Array of Independent Disks，独立磁盘冗余阵列），此处的RAID分类基于Disk RAID进行描述，Disk RAID具备多种分类和组合方式，其中比较常见的Disk RAID Level分类如下

- RAID 0（条带化）

    正如RAID其名称，冗余意味着一套系统具备两套设备用于实现同样的功能，当正在工作的这套设备故障后，另一套设备可随时接管业务，但RAID 0恰恰是无冗余能力，这也是RAID 0与其他RAID Level本质的区别

    通过一块硬盘组成RIAD 0也是业务中比较常见的一种做法，但想要实现RAID 0的特性至少需要两块硬盘，多块容量相同的物理硬盘组成一个RAID组，该RAID组的级别是RAID 0。一般情况下，硬盘提供串行I/O能力，这意味着I/O的数据是串行的逐个处理的，而RAID 0将多个物理硬盘条带化，数据会被拆分为块并均匀分布到多个硬盘中，因此RAID 0所有的I/O操作都会使RAID 0下的物理硬盘同时工作，实现了上层的并行读写数据

- RAID 1（镜像）

    RAID 1必须由偶数个硬盘组成，RAID 1的本质是一种镜像技术，RAID 1的I/O操作会将同样的数据写入两块不同的硬盘，即数据写入硬盘时会完全复制到另一磁盘（镜像，也称为双写），读取数据时两块硬盘中的任意一块都可以提供服务。RAID 1通过双写实现了100%的冗余率，同样也因为双写导致硬盘容量的利用率只有50%

- RAID 5（分布式奇偶校验）

    RAID 5至少需要3块硬盘，是`n+1`校验技术，数据分条存储，奇偶校验信息分散在所有磁盘中，但至少需要拿出1块硬盘的空间用于存储校验码

    RAID 5的硬盘空间利用率取决于`n+1`中的`n`，以3块300G的硬盘组成的RAID 5为例，`n=2`则代表该RAID 5的可用硬盘空间是`n*300=2*300=600G`，也就是说真正的业务数据可以写入600G的硬盘容量，剩下的一块硬盘的空间用于存放存储校验码，存储校验码是根据真实的业务数据计算得出的。当写入业务数据的两块硬盘中的任意一块硬盘损坏时，都可以通过另外一块硬盘的业务数据配合存储校验码，重新计算出损坏的业务数据，如此可以恢复完整的业务数据

    需要注意的是，存储校验码并不是固定放在某一块硬盘上的，它会分布在所有物理硬盘成员上

- RAID 6（双重奇偶校验）

    RAID 6至少需要4块硬盘，号称`n+2`校验技术，与RAID 5类似，但存储两组独立奇偶校验。相比较RAID 5只允许损坏一块硬盘，RAID 6可容忍双硬盘故障，但相应的RAID 6需要拿出2块盘的空间用于存储校验码，与RAID 5相同的是，RAID 6的可用硬盘空间也是`n`

    相比较RAID 1，无论是RAID 5或RAID 6都存在一定的计算开销，存储校验码需要通过CPU参与计算。因此服务器中应尽量使用硬件的RAID卡，存储校验码的计算、数据的读写操作都可以通过RAID卡实现，减少CPU的参与，提升读写性能

- RAID 10（混合级别，RAID 1+0）

    先镜像RAID 1再条带化RAID 0，基于镜像冗余的条带化组合。如果要实现RAID 1和RAID 0的特性，则RAID 10至少需要4块硬盘。由于RAID 10在RAID组间实现并行读写，因此RAID 10的性能会比RAID 1性能更好，RAID 50同样如此

- RAID 50（混合级别，RAID 5+0）

    先分布式奇偶校验RAID 5再条带化RAID 0，基于分布式奇偶校验的条带化组合。如果要实现RAID 5和RAID 0的特性，则RAID 50至少需要6块硬盘。所有的混合级别都是最后做RAID 0，这是因为RAID 0是所有RAID Level中I/O速率最快的，先通过RAID 1或RAID 5实现组内数据冗余，然后通过RAID 0实现组间I/O速率最大化

RAID Level还有`RAID 2`、`RAID 3`、`RAID 4`，只不过这些级别的RAID组在存储服务器上不常用。

| RAID级别 | 优势 | 劣势 | 应用场景 |
| :-- | :-- | :-- | :-- |
| RAID 0 | 读写速度最快，硬盘容量利用率100% | 无冗余，任一硬盘损坏都会导致全部数据丢失 | 临时数据处理、视频渲染等对速度要求高但对安全性要求低的场景 |
| RAID 1 | 高数据安全性，读取性能提升，可并行读取镜像数据 | 硬盘容量利用率50%，成本较高 | 安装操作系统、存放关键事务日志（如银行系统） |
| RAID 5 | 兼顾性能与冗余，支持单硬盘故障恢复 | 因需计算奇偶校验码，导致写入速度较慢，重建数据时性能也会下降 | 需要平衡性能与安全的场景 |
| RAID 6 | 更高的容错能力 | 写入性能更低，成本更高 | 需要平衡性能与安全的场景 |

## 存储方案

存储方案主要针对共享存储进行描述，共享存储可以被主机集群中的多个主机共享，满足数据中心下多个服务器将数据集中存放到某个共享存储的场景，存储方案分为两大类：集中式存储方案、分布式存储方案，其中分布式存储方案是目前比较具备挑战性和前瞻性的

### 集中式存储

集中式存储分2类：NAS（Network Attached Storage，网络附加存储）、SAN（Storage Area Network，存储区域网络），这2种集中式存储也是传统存储中最典型的两种共享存储

**NAS**

NAS服务端可以由Windows或类Unix服务器提供的服务，但无论是哪种类型的服务器，NAS最终都是为前端主机提供了一个用户访问的文件接口，即共享的文件系统存储。因此前端主机访问NAS服务时也需要通过文件共享协议实现，目前最常见的两类文件共享协议分别是`CIFS（Samba）`、`NFS`，其中CIFS协议由微软开发，在Windows服务端上比较常见，NFS协议最初就是为了解决类Unix系统之间的文件共享问题，因此在类Unix服务端比较常见。但无论是哪种文件共享协议，在两种服务端上都是可以运行的

NAS的应用场景一般是中小型企业做文件共享使用，因此NAS提供的是**应用级别、文件级别**的读写，一般情况下NAS需要先在服务端上创建一个目录，然后通过CIFS或NFS协议将该目录共享到计算节点，而SAN与NAS的应用场景不一样，两者提供的服务模式也不一样

> 补充：文件共享协议与文件传输协议的区别
>
>文件共享协议（CIFS、NFS）要区别于文件传输协议（FTP、HTTP），文件共享协议可以做在线的读写操作，传输协议需要先下载，读写操作后再上传

**SAN**

SAN提供**块级别**的访问，简单理解就是对硬盘上的扇区执行读写操作，因此SAN所提供的服务是让前端主机得到一块逻辑硬盘的过程。当业务数据存放在计算节点本地的硬盘上时可能会出现一个问题，当计算节点故障时，故障机器本地的业务数据，无法被接管相同业务的其他计算节点读取，因此业务数据不应该存放在容易出现故障的计算节点上。共享存储的出现就是为了避免因计算节点的损坏而导致本地业务数据的丢失，多个计算节点的数据会集中存放在一个后端存储上，即阵列

I/O数据在应用层的协议是SCSI协议，一般情况下计算节点通过SCSI协议访问本地硬盘，当计算节点原本从访问本地硬盘变更为访问远端阵列的硬盘后，I/O的路径变长可能会导致时延增长，进而影响到正常业务的体验。因此，计算节点与后端存储之间的网络不能是普通的IP网络，这个网络上传输的是对丢包相当敏感的I/O数据、存储数据，对这个网络的要求必须是高质量、不丢包的网络。把计算节点跟阵列之间用高速、低时延、高吞吐的网络互联，要实现计算节点访问远端阵列上的硬盘所花费的时延，跟计算节点访问本地硬盘所花费的时延相当，如此才不会影响正常的业务体验

跟以太网同期诞生的由服务器IT厂商设计的一套网络技术标准：Fiber Channel，简称FC。FC网络是一个具备5层专有协议栈的专网，从诞生之初就是高速、不丢包的网络，用于传输“SAN数据”，即计算节点与存储之间的I/O数据。当计算节点的I/O数据的操作对象从本地硬盘变更为后端存储后，计算节点仍需通过应用层的SCSI协议，像访问自己本地硬盘一样访问后端存储。但现在由于计算节点与存储介质分开放在不同的位置，因此计算节点与存储之间的网络还需要一个协议负责承载SCSI的块访问协议，即FC。如今，计算节点本地的硬盘仅用于安装操作系统，数据则写入远端阵列的硬盘上，此时，如果计算节点与后端存储之间需要使用FC线路通信，保证数据的低时延、不丢包，这个区域的网络就叫**FC SAN**

FC具备几大特点：

1. 光纤作为传输介质
2. 访问速率是2的x次幂
3. 不丢包
4. 昂贵
5. 封闭网络；FC网络上只传递SCSI协议的块访问数据，即SAN存储数据

在特殊场景下，如金融、证券等重要数据业务场景下，必须使用*FC SAN*以保证数据的不丢失，然而大多数场景下*FC SAN*的成本都显得太过高昂，同时随着以太网的高速发展，如今的以太网在速率上已经可以满足更多的业务需求，结合光纤作为传输介质，间接性的缓解了以太网丢包的问题，也就是说以太网也具备了提供很好的传输I/O数据的能力，因此以太网也可以作为应用在计算节点与存储之间的传输网络，由此产生了更经济型的共享SAN存储，即**IP SAN**。IP SAN将计算节点与后端存储之间的线路由FC替换为Ethernet。FC有自己的1~4层的协议栈，SCSI耦合在FC上。**IP SAN**中使用TCP/IP协议栈，SCSI耦合在**TCP/IP协议栈**上，为了区分FC SAN和IP SAN，将IP SAN上的SCSI协议也称为**iSCSI**

光纤本身的抗干扰性强，且出现拥塞、丢包的情况相比较网线而言会好很多，除了光纤本身存在质量问题以外，大多数情况下的IP SAN网络问题都是因为网络设备性能不足、光模块故障等问题导致的，SAN网络独立于整体的业务网络拓扑，其本身并不复杂，因此拥塞、丢包的可能性进一步降低。但无论技术发展的多么成熟，以太网本质上还是一个会丢包的网络，即便具备高速和光纤的加持，IP SAN仍无可避免的可能会产生丢包的情况，此时只能由管员手动降低丢包的可能性。即便数据丢包，IP SAN也能够通过TCP/IP协议栈的重传机制保障业务的正常运行，但从稳定性而言，在预算充足的情况下仍建议采用FC SAN

SCSI传输协议原本用于操作系统访问本地硬盘，但在SAN组网架构中，计算节点与后端存储分离，如果计算节点要通过SCSI协议访问后端存储，那么SCSI协议必须被FC或IP所承载。FC SAN的组网与IP SAN的组网不同，IP SAN组网实际上跟以太网组网是一样的，通过正常的光纤、网卡接入IP SAN网络即可，FC SAN组网对计算节点的硬件有要求，计算节点必须具备HBA（Host Bus Adapter，主机总线适配器）卡，网络中使用的网络设备也必须是FC交换机

#### SAN阵列

![SAN阵列](/image/HCIA/SAN阵列.png)

常见阵列的组成形式分两种：盘控分离、盘控一体。盘控分离的组成部分必须由独立控制框结合硬盘框使用，单独仅使用其中一个都是不行的，由于独立控制框设备内部本身还存在较大空间，因此衍生出盘控一体控制框，盘控一体控制框内置了一些硬盘，可以直接作为独立的设备使用，中小企业采购阵列时，如果对硬盘的数量和容量要求不高的情况下，一般会优先考虑盘控一体控制框，有硬盘扩容需求时，也可以通过盘控一体控制框继续级联硬盘框

在前文中有提及，阵列的控制框的控制模块可能会成为性能瓶颈，但也可以通过“扩控集群”的方式缓解性能问题，控制框上的级联模块不仅可以用于连接硬盘框扩容硬盘容量，也可以用于连接新的控制框用于扩控，多个控制框下再级联硬盘框

#### LUN

相比较NAS文件系统级别的共享，SAN实现共享存储的方式则完全不一样，无论前端的计算节点是否为CNA节点，如果要使用后端阵列提供的SAN存储，都需要经过几个步骤：

1. 在阵列初始化时划分**硬盘域**
2. 在硬盘域中划分**块存储池**
3. 在存储池中划分**LUN**（Logical Unit Number）
4. 将LUN映射给计算节点

LUN可以理解为一个逻辑块设备，在`RAID 2.0+`技术的加持下，一个LUN的容量实际上可能由阵列上的多块物理硬盘组成，计算节点在使用LUN时屏蔽了底层的复杂结构，实际上计算节点最终访问的设备也就是阵列上的LUN。在阵列上划分好LUN之后还需要通过阵列的操作系统将LUN映射给指定的计算节点，每个计算节点只能发现并访问到映射到该节点的LUN。以FC协议为例，从计算节点访问到阵列上的LUN，这整个过程都需要通过上层的SCSI协议实现，LUN都是SCSI设备

如果底层的承载协议是TCP/IP协议，即iSCSI，计算节点与存储阵列之间首选需要通过IP地址实现互联，其次再通过SCSI协议访问存储，SCSI协议本身也有其自己的地址，被称为**IQN**，通过SCSI协议访问存储时必须提供一个SCSI协议可识别的IQN地址。在忽略底层承载协议的情况下，SCSI协议本身也有被访问端和访问端的概念，被访问端即服务端，又称为**target（目标器）**，访问端即客户端，又称为**SCSI initiator（启动器）**。在阵列上要将LUN映射给主机，则需要在阵列上根据SCSI协议可识别的IQN地址选择initiator，同时主机也会在存储网络中广播自己，以便于被target发现

LUN是向多个计算节点提供的一块远端的、共享的逻辑硬盘，当计算节点拿到这块共享硬盘时，需要向本地硬盘一般，先对硬盘进行格式化文件系统，然后才能对该硬盘读写文件。阵列上可以划分多个LUN，不同的LUN可以用于承担不同的业务数据

> **补充：存储操作**
>
> 在存储设备上可以修改或删除主机组、LUN组、映射关系配置，但对LUN的操作一定是高危操作，删除LUN之前必须拷贝数据，否则数据会随着LUN的删除而丢失
> 在存储阵列上创建LUN时，可以选择是否启用`thin LUN`，缺省情况下使用的是`think LUN`，`thin LUN`也被称为瘦LUN，类似ESXi的精简置备，划分的空间不会立即分配，而是用多少分配多少。`think LUN`也被称为厚LUN，类似ESXi的厚置备置零，划分多少空间就立即分配多少

### 分布式存储

分布式存储又分为3类存储：块存储、文件系统存储、对象存储

分布式存储是将一些零散的服务器，利用软件技术组合到一起通过大容量的存储空间。相比较阵列而言，分布式存储可以通过添加服务器来实现存储池的扩容，因此分布式存储的扩容相比较阵列而言也相对容易。最重要的一点是，即便阵列控制框可以扩容，控制框扩容的数量、控制框提供的I/O接口的带宽都是有限的，因此阵列的控制框会成为性能瓶颈

相比较阵列，分布式存储是**去中心化**的，能够提供高性能的**并行I/O访问**，例如，某一个计算节点的数据在写入存储之前，会对该数据进行切片，数据会被打散均衡的写入不同的后端存储服务器。每个计算节点都有自己的CPU、内存、网卡等硬件设备，每台服务器都能独立的对外提供I/O读写，因此分布式存储不存在机头瓶颈这种问题。分布式存储横向扩展容易，服务器的成本也比阵列低，但服务器不是专业的阵列，分布式存储的所有解决方案都是通过软件实现的，因此相比较阵列，分布式存储还需要解决数据的可靠性、可用性的问题，为了避免出现数据丢失的情况，需要**副本**机制保证数据完整性，但副本越多，可用空间越小

> **补充：可靠性与可用性**
>
> 可靠性：避免故障的发生
>
> 可用性：当故障发生时，确保业务仍能够正常运转
>
> 可靠性与可用性的区别：可用性主要实现业务连续，也就是故障发生时能够实现业务不中断或极短时间内恢复业务的特性。可靠性主要站在预防故障的视角
>
> Fusion Storage：华为的分布式存储软件

## 虚拟化存储

虚拟化存储小节中需要讨论两个问题：什么是虚拟化存储、什么是数据存储。数据存储不代表硬盘之类的存储介质，它是华为产品中的一个专业术语，指的是为VM提供vDisk空间的存储就称为数据存储，在Hypervisor中指的是可管理、操作的存储逻辑单元

以SAN组网架构为例，存储阵列与CNA互联，在阵列上是将LUN映射给CNA节点，对于前端的多个CNA节点而言，LUN就是一块共享的逻辑硬盘，由CNA作为启动器发起SCSI请求，与VM没有直接的关系。在创建、配置虚拟机时，CNA节点作为Hypervisor必须为虚拟机提供vDisk，同时，如果CNA要向虚拟机提供vDisk，那么CNA本身必须先具备**数据存储**，换言之，CNA上所有的虚拟机的vDisk最终都会落在CNA的数据存储上

此时，后端存储阵列的LUN作为CNA节点上的一块逻辑硬盘，那么，将LUN转换为数据存储，即可实现虚拟机的vDisk数据最终落在后端存储阵列上，如果多个CNA节点的数据存储都使用同一个LUN，这就意味着每个CNA节点都能够访问到所有的虚拟机的vDisk，这种方式极大的便利于虚拟机的迁移，只需要在其他CNA节点上提供虚拟机所需的计算资源即可实现迁移

由后端存储提供的SAN的LUN、NAS的文件系统、FusionStorage的存储池，包括CNA本地的硬盘都能被转换为数据存储，只不过不建议使用CNA本地的硬盘，安全性没有保障的同时，也不便于虚拟机的迁移

### 数据存储配置

到目前为止应该明确，虚拟机的vDisk是基于CNA的数据存储的，CNA不具备数据存储的情况下，无法正常创建虚拟机。因此首先需要为CNA节点先添加一个数据存储，缺省情况下每个CNA节点至少是能够扫描到自身本地硬盘的，只不过本地硬盘无法提供给其他CNA节点共用

![无数据存储](/image/HCIA/无数据存储.png)

1. [存储] -> [存储设备] -> [扫描]；使CNA节点先通过自身的存储接口扫描一下网络中所有能被发现的存储设备，正常情况下至少应该能扫到各个CNA节点本地的硬盘

    ![创建数据存储-1](/image/HCIA/创建数据存储-1.png)

2. [存储] -> [存储设备] -> [查看已关联主机]；扫描出的本地硬盘是直接与CNA节点相关联的



3. [CNA01] -> [数据存储] -> [添加数据存储] -> [勾选本地硬盘] -> [编辑数据存储的基本信息]；为CNA节点将本地硬盘添加为数据存储



    在添加数据存储时需要选择数据存储的使用方式是否为虚拟化数据存储，CNA的数据存储分为两种类型：**虚拟化数据存储/非虚拟化数据存储**

4. [CNA01] -> [创建虚拟机] -> [基本配置] -> [选择数据存储] -> [虚拟机配置] -> [磁盘1]；查看创建虚拟机过程中，vDisk存储的落点，如果CNA节点上存在多个数据存储，则在创建虚拟机时还可以选择vDisk落在指定的数据存储上



在VRM的Web管理视图页面，存储菜单下面又分为3个子栏目：数据存储、存储设备、存储资源。在上文的描述中一直在强调数据存储的重要性，实际上三者负责的内容各不相同

- 数据存储：数据存储指的是虚拟化平台（Hypervisor）中可管理、操作的存储逻辑单元，为VM提供vDisk的存储空间
- 存储设备：存储设备表示存储资源中的管理单元，类似LUN、FusionStorage存储池、NAS共享目录等
- 存储资源：存储资源表示物理存储设备，例如IP SAN、FC SAN、NAS等

![华为虚拟化产品存储架构](/image/HCIA/华为虚拟化产品存储架构.png)

除了存储资源直接指代物理设备，数据存储、存储设备两者都是逻辑概念。

### 虚拟化数据存储与非虚拟化数据存储

从SAN存储划分LUN、再从LUN添加为数据存储的过程中，在添加数据存储时需要选择该数据存储的类型，由于LUN的特性，它在CNA节点上显示为一个块设备，即硬盘，一块硬盘必须经过格式化之后才能读写文件，格式化就意味着必须在该LUN上先建立文件系统，这也是虚拟化数据存储的主要特征。虚拟化数据存储的主要特征是具备文件系统，由CNA格式化后的文件系统，华为官方称为VIMS v2（虚拟镜像管理系统），VIMS基于OCFS改写，是高性能集群文件系统

由NAS存储提供的共享存储则无需执行格式化步骤，因为NAS本身提供的就是文件系统级别的共享，在NAS存储上本身就具备文件系统，CNA节点上添加的数据存储也只是一个可读写的目录而已，基于虚拟化数据存储的特征，NAS提供的共享存储也是虚拟化数据存储，但将NAS存储添加为CNA数据存储时仅支持NFSv3协议。由FusionStorage提供的分布式存储没有文件系统，属于非虚拟化存储。将CNA本地硬盘添加为数据存储同样是虚拟化存储，因为CNA本地硬盘上同样存在文件系统Ext4

虚拟化数据存储与非虚拟化数据存储的区别在于，虚拟化数据存储一定是具备文件系统的，那么vDisk也会以“文件”的方式存放在虚拟化数据存储的文件系统上，根据虚文件系统的不同，vDisk也会以不同的**文件格式（形态）**存放。非虚拟化数据存储因为没有文件系统，vDisk在非虚拟化数据存储上以**卷（形态）**的方式存放。两种数据存储上的vDisk的形态不同，文件形态的vDisk支持更多的虚拟化高级特性，虚拟化数据存储能够提供五大虚拟化特性：磁盘自动精简配置、快照、链接克隆、磁盘扩容、存储热迁移，非虚拟化数据存储仅支持前3个特性；卷形态的vDisk相对而言I/O性能更好，因为非虚拟化数据存储中间减少了一个文件系统层

无论是虚拟化数据存储或非虚拟化数据存储，生成这两种数据存储的过程被称为存储虚拟化（动词）

### 共享存储配置

通过额外配置一个iSCSI存储和NFS存储来分别演示FusionCompute接入SAN和NAS共享存储的配置过程，iSCSI和NFS的配置既可以采用Linux系统配置也可以使用Windows系统配置，此处不做共享存储配置的演示

实验信息：

| 节点 | 业务网络 | 存储网络 |
| :-- | :-- | :-- |
| CNA01 | 192.168.122.10 | 192.168.2.10 |
| CNA02 | 192.168.122.11 | 192.168.2.11 |
| CNA03 | 192.168.122.12 | 192.168.2.12 |
| Storage | / | 192.168.2.250 |










但此处通过VMware做嵌套虚拟化实验，没有扫描除本地硬盘，即便额外再在VMware上为虚拟机添加硬盘也扫描不出来，因此实验环境中单独配置了一套iSCSI target端作为存储







#### 虚拟化数据存储与非虚拟化数据存储的区别



**非虚拟化数据存储**

非虚拟化数据存储无文件系统，vDISK以**卷**的方式存放

```diff
- 存储虚拟化：动词，生成虚拟化数据存储或非虚拟化数据存储的过程叫做存储虚拟化
- 虚拟化存储会提供五大虚拟化特性：磁盘自动精简配置、快照、链接克隆、磁盘扩容、存储热迁移，非虚拟化存储仅支持前3个特性

+ 非虚拟化存储的I/O性能更好，因为中间少了一个文件系统层
```

#### 华为虚拟化磁盘特性

**类型**

- 普通：一盘一机
- 共享：一盘多机

**配置模式**

- 普通：为VM配置多大的容量，则立即在LUN上划分多大容量
- 精简：按VM实际的磁盘使用情况在LUN上划分容量分配

**磁盘模式**

- 从属：快照时对当前磁盘不做快照
- 独立-持久：独立表示快照时对当前磁盘也做快照，持久表示数据永久写入磁盘
- 独立-非持久：非持久表示数据写入磁盘时，重启系统数据就没了

```diff
+ 为VM配置vDISK时，除了普通和精简两种配置模式之外还有一种"普通延时置零"，其与普通模式的区别在于不是立即将所有数据擦除置零，初次使用性能强于普通模式
```

**最佳实践：SAN**

1. 在VRM下点击“CNA”，在“配置”栏下点击“网络”-“逻辑接口”，“添加存储接口”或“添加业务管理口”。此处为CNA配置不同的网络平面接口

2. 在VRM下点击“存储”，在“存储资源”栏下点击“添加存储资源”，选择存储资源类型，设置存储IP，设置关联主机。“IPSAN”类型的存储资源的管理IP是*不必要*的，可以随意设置。关联主机表示可使用磁存储的主机

3. 在VRM下点击“存储”，在“存储设备”栏下点击“扫描”

4. 在VRM下点击“存储”，在“数据存储”栏下点击“添加数据存储”，在子菜单“添加数据存储”下选择使用方式

```diff
- FC的组网架构中分为三个网络平面：业务网络、管理网络、存储网络
- 添加数据存储时有两种使用方式：虚拟化、裸设备映射。虚拟化为所有VM提供vDISK空间，裸设备映射表示将存储直通给某个VM，仅为一台VM提供存储

+ 在VRM上添加“数据存储”后，可以在存储上“创建磁盘”，此创建的磁盘类似U盘的概念，这个磁盘可以后期“绑定”到VM中
+ 删除存储资源与添加存储资源过程相反
```

**最佳实践：删除存储资源**

1. 删除“数据存储”，删除“数据存储”前必须保证此数据存储中的数据都清空了，否则无法删除

2. 在“存储资源”栏下断开“关联主机”，删除“存储资源”

```diff
+ 存储设备都是扫描发现的，所以没有删除操作
```

